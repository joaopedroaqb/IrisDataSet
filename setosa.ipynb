{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "619Fcv2Zoe3h"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "novoArray = np.random.rand(5,5)\n",
        "print(novoArray)\n",
        "print(type(novoArray))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHQKnJfVoqb8",
        "outputId": "f2e4f43b-a454-424e-b3f4-ccafea4fa60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.85454847 0.76454359 0.97530281 0.61884793 0.23491156]\n",
            " [0.29442295 0.04959454 0.20776621 0.22397552 0.48288803]\n",
            " [0.95667149 0.23526701 0.03002837 0.68619741 0.06541342]\n",
            " [0.32942691 0.94097527 0.08604551 0.79568197 0.32469605]\n",
            " [0.86553219 0.13343092 0.39104391 0.3517107  0.24788871]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor vs Array\n",
        "novoTensor = torch.rand(5,5)\n",
        "print(novoTensor) \n",
        "print(type(novoTensor)) \n",
        "print(novoArray)\n",
        "print(type(novoArray))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMfvca8joqeq",
        "outputId": "7cb2c85e-67d1-46d3-b0c9-bde5af3e741a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3955, 0.2166, 0.3719, 0.3491, 0.4129],\n",
            "        [0.2211, 0.6958, 0.5230, 0.7464, 0.2120],\n",
            "        [0.7076, 0.9567, 0.2121, 0.2335, 0.1603],\n",
            "        [0.4475, 0.0746, 0.4374, 0.6550, 0.5900],\n",
            "        [0.0335, 0.1256, 0.5364, 0.2038, 0.7540]])\n",
            "<class 'torch.Tensor'>\n",
            "[[0.85454847 0.76454359 0.97530281 0.61884793 0.23491156]\n",
            " [0.29442295 0.04959454 0.20776621 0.22397552 0.48288803]\n",
            " [0.95667149 0.23526701 0.03002837 0.68619741 0.06541342]\n",
            " [0.32942691 0.94097527 0.08604551 0.79568197 0.32469605]\n",
            " [0.86553219 0.13343092 0.39104391 0.3517107  0.24788871]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um modelo de Rede Neural\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Net, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size) #full connected\n",
        "    self.relu = torch.nn.ReLU() #(0, infinito)\n",
        "    self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "    self.sigmoid = torch.nn.Sigmoid() #(0, 1)\n",
        "  def forward(self, x):\n",
        "    hidden = self.fc1(x)\n",
        "    relu = self.relu(hidden)\n",
        "    output = self.fc2(relu)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "VOJvDjlPouuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "# data, target, target_names\n",
        "# print(iris)\n",
        "dados = iris.data\n",
        "classes = iris.target\n",
        "nomesClasses = iris.target_names"
      ],
      "metadata": {
        "id": "uT9VwYqrozrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter as classes\n",
        "print(classes)\n",
        "print(nomesClasses)\n",
        "import numpy as np\n",
        "# saida = np.where(condição, true, false)\n",
        "# saida = np.where(classes==2, 1, 0) # virginica\n",
        "saida = np.where(classes== 0, 1, 0) #versicolor \n",
        "print(saida)\n",
        "# 0: setosa; 1: versicolor; 2: virginica;\n",
        "# 0: setosa; 0: versicolor; 1: virginica;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEAonZ4go0ac",
        "outputId": "1278f2ae-2602-41ba-d438-0d396eb2ccf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "['setosa' 'versicolor' 'virginica']\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converter para tensor\n",
        "entrada = torch.FloatTensor(dados) / 10\n",
        "saida = torch.FloatTensor(saida)\n",
        "print(torch.max(entrada))\n",
        "print(saida)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Uvv_piqJwZ",
        "outputId": "948fbad5-9c23-4e6c-e221-69469bc28da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7900)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "print(entrada, saida)\n",
        "entrada, saida = shuffle(entrada, saida)\n",
        "print(entrada, saida)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojUIQTmkqKuQ",
        "outputId": "810f2654-4a69-49dc-ae12-9ddfe84fe1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5100, 0.3500, 0.1400, 0.0200],\n",
            "        [0.4900, 0.3000, 0.1400, 0.0200],\n",
            "        [0.4700, 0.3200, 0.1300, 0.0200],\n",
            "        [0.4600, 0.3100, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3600, 0.1400, 0.0200],\n",
            "        [0.5400, 0.3900, 0.1700, 0.0400],\n",
            "        [0.4600, 0.3400, 0.1400, 0.0300],\n",
            "        [0.5000, 0.3400, 0.1500, 0.0200],\n",
            "        [0.4400, 0.2900, 0.1400, 0.0200],\n",
            "        [0.4900, 0.3100, 0.1500, 0.0100],\n",
            "        [0.5400, 0.3700, 0.1500, 0.0200],\n",
            "        [0.4800, 0.3400, 0.1600, 0.0200],\n",
            "        [0.4800, 0.3000, 0.1400, 0.0100],\n",
            "        [0.4300, 0.3000, 0.1100, 0.0100],\n",
            "        [0.5800, 0.4000, 0.1200, 0.0200],\n",
            "        [0.5700, 0.4400, 0.1500, 0.0400],\n",
            "        [0.5400, 0.3900, 0.1300, 0.0400],\n",
            "        [0.5100, 0.3500, 0.1400, 0.0300],\n",
            "        [0.5700, 0.3800, 0.1700, 0.0300],\n",
            "        [0.5100, 0.3800, 0.1500, 0.0300],\n",
            "        [0.5400, 0.3400, 0.1700, 0.0200],\n",
            "        [0.5100, 0.3700, 0.1500, 0.0400],\n",
            "        [0.4600, 0.3600, 0.1000, 0.0200],\n",
            "        [0.5100, 0.3300, 0.1700, 0.0500],\n",
            "        [0.4800, 0.3400, 0.1900, 0.0200],\n",
            "        [0.5000, 0.3000, 0.1600, 0.0200],\n",
            "        [0.5000, 0.3400, 0.1600, 0.0400],\n",
            "        [0.5200, 0.3500, 0.1500, 0.0200],\n",
            "        [0.5200, 0.3400, 0.1400, 0.0200],\n",
            "        [0.4700, 0.3200, 0.1600, 0.0200],\n",
            "        [0.4800, 0.3100, 0.1600, 0.0200],\n",
            "        [0.5400, 0.3400, 0.1500, 0.0400],\n",
            "        [0.5200, 0.4100, 0.1500, 0.0100],\n",
            "        [0.5500, 0.4200, 0.1400, 0.0200],\n",
            "        [0.4900, 0.3100, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3200, 0.1200, 0.0200],\n",
            "        [0.5500, 0.3500, 0.1300, 0.0200],\n",
            "        [0.4900, 0.3600, 0.1400, 0.0100],\n",
            "        [0.4400, 0.3000, 0.1300, 0.0200],\n",
            "        [0.5100, 0.3400, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3500, 0.1300, 0.0300],\n",
            "        [0.4500, 0.2300, 0.1300, 0.0300],\n",
            "        [0.4400, 0.3200, 0.1300, 0.0200],\n",
            "        [0.5000, 0.3500, 0.1600, 0.0600],\n",
            "        [0.5100, 0.3800, 0.1900, 0.0400],\n",
            "        [0.4800, 0.3000, 0.1400, 0.0300],\n",
            "        [0.5100, 0.3800, 0.1600, 0.0200],\n",
            "        [0.4600, 0.3200, 0.1400, 0.0200],\n",
            "        [0.5300, 0.3700, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3300, 0.1400, 0.0200],\n",
            "        [0.7000, 0.3200, 0.4700, 0.1400],\n",
            "        [0.6400, 0.3200, 0.4500, 0.1500],\n",
            "        [0.6900, 0.3100, 0.4900, 0.1500],\n",
            "        [0.5500, 0.2300, 0.4000, 0.1300],\n",
            "        [0.6500, 0.2800, 0.4600, 0.1500],\n",
            "        [0.5700, 0.2800, 0.4500, 0.1300],\n",
            "        [0.6300, 0.3300, 0.4700, 0.1600],\n",
            "        [0.4900, 0.2400, 0.3300, 0.1000],\n",
            "        [0.6600, 0.2900, 0.4600, 0.1300],\n",
            "        [0.5200, 0.2700, 0.3900, 0.1400],\n",
            "        [0.5000, 0.2000, 0.3500, 0.1000],\n",
            "        [0.5900, 0.3000, 0.4200, 0.1500],\n",
            "        [0.6000, 0.2200, 0.4000, 0.1000],\n",
            "        [0.6100, 0.2900, 0.4700, 0.1400],\n",
            "        [0.5600, 0.2900, 0.3600, 0.1300],\n",
            "        [0.6700, 0.3100, 0.4400, 0.1400],\n",
            "        [0.5600, 0.3000, 0.4500, 0.1500],\n",
            "        [0.5800, 0.2700, 0.4100, 0.1000],\n",
            "        [0.6200, 0.2200, 0.4500, 0.1500],\n",
            "        [0.5600, 0.2500, 0.3900, 0.1100],\n",
            "        [0.5900, 0.3200, 0.4800, 0.1800],\n",
            "        [0.6100, 0.2800, 0.4000, 0.1300],\n",
            "        [0.6300, 0.2500, 0.4900, 0.1500],\n",
            "        [0.6100, 0.2800, 0.4700, 0.1200],\n",
            "        [0.6400, 0.2900, 0.4300, 0.1300],\n",
            "        [0.6600, 0.3000, 0.4400, 0.1400],\n",
            "        [0.6800, 0.2800, 0.4800, 0.1400],\n",
            "        [0.6700, 0.3000, 0.5000, 0.1700],\n",
            "        [0.6000, 0.2900, 0.4500, 0.1500],\n",
            "        [0.5700, 0.2600, 0.3500, 0.1000],\n",
            "        [0.5500, 0.2400, 0.3800, 0.1100],\n",
            "        [0.5500, 0.2400, 0.3700, 0.1000],\n",
            "        [0.5800, 0.2700, 0.3900, 0.1200],\n",
            "        [0.6000, 0.2700, 0.5100, 0.1600],\n",
            "        [0.5400, 0.3000, 0.4500, 0.1500],\n",
            "        [0.6000, 0.3400, 0.4500, 0.1600],\n",
            "        [0.6700, 0.3100, 0.4700, 0.1500],\n",
            "        [0.6300, 0.2300, 0.4400, 0.1300],\n",
            "        [0.5600, 0.3000, 0.4100, 0.1300],\n",
            "        [0.5500, 0.2500, 0.4000, 0.1300],\n",
            "        [0.5500, 0.2600, 0.4400, 0.1200],\n",
            "        [0.6100, 0.3000, 0.4600, 0.1400],\n",
            "        [0.5800, 0.2600, 0.4000, 0.1200],\n",
            "        [0.5000, 0.2300, 0.3300, 0.1000],\n",
            "        [0.5600, 0.2700, 0.4200, 0.1300],\n",
            "        [0.5700, 0.3000, 0.4200, 0.1200],\n",
            "        [0.5700, 0.2900, 0.4200, 0.1300],\n",
            "        [0.6200, 0.2900, 0.4300, 0.1300],\n",
            "        [0.5100, 0.2500, 0.3000, 0.1100],\n",
            "        [0.5700, 0.2800, 0.4100, 0.1300],\n",
            "        [0.6300, 0.3300, 0.6000, 0.2500],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.7100, 0.3000, 0.5900, 0.2100],\n",
            "        [0.6300, 0.2900, 0.5600, 0.1800],\n",
            "        [0.6500, 0.3000, 0.5800, 0.2200],\n",
            "        [0.7600, 0.3000, 0.6600, 0.2100],\n",
            "        [0.4900, 0.2500, 0.4500, 0.1700],\n",
            "        [0.7300, 0.2900, 0.6300, 0.1800],\n",
            "        [0.6700, 0.2500, 0.5800, 0.1800],\n",
            "        [0.7200, 0.3600, 0.6100, 0.2500],\n",
            "        [0.6500, 0.3200, 0.5100, 0.2000],\n",
            "        [0.6400, 0.2700, 0.5300, 0.1900],\n",
            "        [0.6800, 0.3000, 0.5500, 0.2100],\n",
            "        [0.5700, 0.2500, 0.5000, 0.2000],\n",
            "        [0.5800, 0.2800, 0.5100, 0.2400],\n",
            "        [0.6400, 0.3200, 0.5300, 0.2300],\n",
            "        [0.6500, 0.3000, 0.5500, 0.1800],\n",
            "        [0.7700, 0.3800, 0.6700, 0.2200],\n",
            "        [0.7700, 0.2600, 0.6900, 0.2300],\n",
            "        [0.6000, 0.2200, 0.5000, 0.1500],\n",
            "        [0.6900, 0.3200, 0.5700, 0.2300],\n",
            "        [0.5600, 0.2800, 0.4900, 0.2000],\n",
            "        [0.7700, 0.2800, 0.6700, 0.2000],\n",
            "        [0.6300, 0.2700, 0.4900, 0.1800],\n",
            "        [0.6700, 0.3300, 0.5700, 0.2100],\n",
            "        [0.7200, 0.3200, 0.6000, 0.1800],\n",
            "        [0.6200, 0.2800, 0.4800, 0.1800],\n",
            "        [0.6100, 0.3000, 0.4900, 0.1800],\n",
            "        [0.6400, 0.2800, 0.5600, 0.2100],\n",
            "        [0.7200, 0.3000, 0.5800, 0.1600],\n",
            "        [0.7400, 0.2800, 0.6100, 0.1900],\n",
            "        [0.7900, 0.3800, 0.6400, 0.2000],\n",
            "        [0.6400, 0.2800, 0.5600, 0.2200],\n",
            "        [0.6300, 0.2800, 0.5100, 0.1500],\n",
            "        [0.6100, 0.2600, 0.5600, 0.1400],\n",
            "        [0.7700, 0.3000, 0.6100, 0.2300],\n",
            "        [0.6300, 0.3400, 0.5600, 0.2400],\n",
            "        [0.6400, 0.3100, 0.5500, 0.1800],\n",
            "        [0.6000, 0.3000, 0.4800, 0.1800],\n",
            "        [0.6900, 0.3100, 0.5400, 0.2100],\n",
            "        [0.6700, 0.3100, 0.5600, 0.2400],\n",
            "        [0.6900, 0.3100, 0.5100, 0.2300],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.6800, 0.3200, 0.5900, 0.2300],\n",
            "        [0.6700, 0.3300, 0.5700, 0.2500],\n",
            "        [0.6700, 0.3000, 0.5200, 0.2300],\n",
            "        [0.6300, 0.2500, 0.5000, 0.1900],\n",
            "        [0.6500, 0.3000, 0.5200, 0.2000],\n",
            "        [0.6200, 0.3400, 0.5400, 0.2300],\n",
            "        [0.5900, 0.3000, 0.5100, 0.1800]]) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n",
            "tensor([[0.5200, 0.3500, 0.1500, 0.0200],\n",
            "        [0.4600, 0.3200, 0.1400, 0.0200],\n",
            "        [0.6800, 0.2800, 0.4800, 0.1400],\n",
            "        [0.6700, 0.2500, 0.5800, 0.1800],\n",
            "        [0.6300, 0.2500, 0.4900, 0.1500],\n",
            "        [0.5800, 0.2800, 0.5100, 0.2400],\n",
            "        [0.6400, 0.3200, 0.4500, 0.1500],\n",
            "        [0.7200, 0.3600, 0.6100, 0.2500],\n",
            "        [0.5100, 0.2500, 0.3000, 0.1100],\n",
            "        [0.4900, 0.3100, 0.1500, 0.0100],\n",
            "        [0.6500, 0.2800, 0.4600, 0.1500],\n",
            "        [0.6300, 0.2900, 0.5600, 0.1800],\n",
            "        [0.7600, 0.3000, 0.6600, 0.2100],\n",
            "        [0.6900, 0.3100, 0.5100, 0.2300],\n",
            "        [0.6100, 0.3000, 0.4900, 0.1800],\n",
            "        [0.7300, 0.2900, 0.6300, 0.1800],\n",
            "        [0.7700, 0.3000, 0.6100, 0.2300],\n",
            "        [0.4900, 0.3100, 0.1500, 0.0200],\n",
            "        [0.4800, 0.3400, 0.1900, 0.0200],\n",
            "        [0.4900, 0.3000, 0.1400, 0.0200],\n",
            "        [0.5900, 0.3000, 0.4200, 0.1500],\n",
            "        [0.6600, 0.3000, 0.4400, 0.1400],\n",
            "        [0.7100, 0.3000, 0.5900, 0.2100],\n",
            "        [0.6000, 0.2200, 0.5000, 0.1500],\n",
            "        [0.5000, 0.3400, 0.1600, 0.0400],\n",
            "        [0.5000, 0.3600, 0.1400, 0.0200],\n",
            "        [0.6300, 0.3400, 0.5600, 0.2400],\n",
            "        [0.5700, 0.2600, 0.3500, 0.1000],\n",
            "        [0.5100, 0.3300, 0.1700, 0.0500],\n",
            "        [0.6200, 0.3400, 0.5400, 0.2300],\n",
            "        [0.5700, 0.2800, 0.4100, 0.1300],\n",
            "        [0.5500, 0.2400, 0.3800, 0.1100],\n",
            "        [0.5400, 0.3000, 0.4500, 0.1500],\n",
            "        [0.5100, 0.3500, 0.1400, 0.0300],\n",
            "        [0.5200, 0.2700, 0.3900, 0.1400],\n",
            "        [0.6100, 0.2800, 0.4700, 0.1200],\n",
            "        [0.6700, 0.3100, 0.5600, 0.2400],\n",
            "        [0.5700, 0.3000, 0.4200, 0.1200],\n",
            "        [0.6000, 0.2200, 0.4000, 0.1000],\n",
            "        [0.6000, 0.2900, 0.4500, 0.1500],\n",
            "        [0.5000, 0.3400, 0.1500, 0.0200],\n",
            "        [0.5600, 0.2700, 0.4200, 0.1300],\n",
            "        [0.6200, 0.2800, 0.4800, 0.1800],\n",
            "        [0.6900, 0.3100, 0.4900, 0.1500],\n",
            "        [0.5400, 0.3400, 0.1700, 0.0200],\n",
            "        [0.6400, 0.2700, 0.5300, 0.1900],\n",
            "        [0.6100, 0.2600, 0.5600, 0.1400],\n",
            "        [0.5800, 0.4000, 0.1200, 0.0200],\n",
            "        [0.4400, 0.3000, 0.1300, 0.0200],\n",
            "        [0.4900, 0.3600, 0.1400, 0.0100],\n",
            "        [0.7400, 0.2800, 0.6100, 0.1900],\n",
            "        [0.4600, 0.3400, 0.1400, 0.0300],\n",
            "        [0.6000, 0.3000, 0.4800, 0.1800],\n",
            "        [0.6300, 0.2300, 0.4400, 0.1300],\n",
            "        [0.7700, 0.2600, 0.6900, 0.2300],\n",
            "        [0.5700, 0.2900, 0.4200, 0.1300],\n",
            "        [0.5600, 0.2500, 0.3900, 0.1100],\n",
            "        [0.6300, 0.3300, 0.6000, 0.2500],\n",
            "        [0.4500, 0.2300, 0.1300, 0.0300],\n",
            "        [0.4700, 0.3200, 0.1600, 0.0200],\n",
            "        [0.5100, 0.3700, 0.1500, 0.0400],\n",
            "        [0.6300, 0.3300, 0.4700, 0.1600],\n",
            "        [0.5600, 0.2800, 0.4900, 0.2000],\n",
            "        [0.4800, 0.3000, 0.1400, 0.0300],\n",
            "        [0.7200, 0.3000, 0.5800, 0.1600],\n",
            "        [0.4800, 0.3400, 0.1600, 0.0200],\n",
            "        [0.5800, 0.2700, 0.3900, 0.1200],\n",
            "        [0.5300, 0.3700, 0.1500, 0.0200],\n",
            "        [0.5400, 0.3400, 0.1500, 0.0400],\n",
            "        [0.5200, 0.3400, 0.1400, 0.0200],\n",
            "        [0.6200, 0.2900, 0.4300, 0.1300],\n",
            "        [0.4900, 0.2500, 0.4500, 0.1700],\n",
            "        [0.4900, 0.2400, 0.3300, 0.1000],\n",
            "        [0.7700, 0.3800, 0.6700, 0.2200],\n",
            "        [0.6400, 0.2800, 0.5600, 0.2100],\n",
            "        [0.5100, 0.3800, 0.1900, 0.0400],\n",
            "        [0.4400, 0.3200, 0.1300, 0.0200],\n",
            "        [0.4800, 0.3000, 0.1400, 0.0100],\n",
            "        [0.5600, 0.3000, 0.4500, 0.1500],\n",
            "        [0.5800, 0.2600, 0.4000, 0.1200],\n",
            "        [0.5000, 0.3200, 0.1200, 0.0200],\n",
            "        [0.5000, 0.3500, 0.1600, 0.0600],\n",
            "        [0.6700, 0.3000, 0.5000, 0.1700],\n",
            "        [0.6400, 0.2800, 0.5600, 0.2200],\n",
            "        [0.6300, 0.2800, 0.5100, 0.1500],\n",
            "        [0.6400, 0.3100, 0.5500, 0.1800],\n",
            "        [0.5700, 0.4400, 0.1500, 0.0400],\n",
            "        [0.4800, 0.3100, 0.1600, 0.0200],\n",
            "        [0.6300, 0.2500, 0.5000, 0.1900],\n",
            "        [0.6700, 0.3300, 0.5700, 0.2100],\n",
            "        [0.6400, 0.2900, 0.4300, 0.1300],\n",
            "        [0.7900, 0.3800, 0.6400, 0.2000],\n",
            "        [0.6500, 0.3000, 0.5800, 0.2200],\n",
            "        [0.4700, 0.3200, 0.1300, 0.0200],\n",
            "        [0.6300, 0.2700, 0.4900, 0.1800],\n",
            "        [0.5000, 0.2000, 0.3500, 0.1000],\n",
            "        [0.4600, 0.3600, 0.1000, 0.0200],\n",
            "        [0.6100, 0.2900, 0.4700, 0.1400],\n",
            "        [0.5500, 0.2600, 0.4400, 0.1200],\n",
            "        [0.6000, 0.2700, 0.5100, 0.1600],\n",
            "        [0.6700, 0.3000, 0.5200, 0.2300],\n",
            "        [0.5400, 0.3900, 0.1300, 0.0400],\n",
            "        [0.6500, 0.3200, 0.5100, 0.2000],\n",
            "        [0.5100, 0.3400, 0.1500, 0.0200],\n",
            "        [0.5500, 0.2300, 0.4000, 0.1300],\n",
            "        [0.7700, 0.2800, 0.6700, 0.2000],\n",
            "        [0.6700, 0.3300, 0.5700, 0.2500],\n",
            "        [0.5700, 0.2500, 0.5000, 0.2000],\n",
            "        [0.5600, 0.3000, 0.4100, 0.1300],\n",
            "        [0.5100, 0.3800, 0.1500, 0.0300],\n",
            "        [0.4400, 0.2900, 0.1400, 0.0200],\n",
            "        [0.5000, 0.3500, 0.1300, 0.0300],\n",
            "        [0.6400, 0.3200, 0.5300, 0.2300],\n",
            "        [0.4300, 0.3000, 0.1100, 0.0100],\n",
            "        [0.6800, 0.3200, 0.5900, 0.2300],\n",
            "        [0.5400, 0.3700, 0.1500, 0.0200],\n",
            "        [0.5600, 0.2900, 0.3600, 0.1300],\n",
            "        [0.5800, 0.2700, 0.4100, 0.1000],\n",
            "        [0.5700, 0.2800, 0.4500, 0.1300],\n",
            "        [0.6000, 0.3400, 0.4500, 0.1600],\n",
            "        [0.4600, 0.3100, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3000, 0.1600, 0.0200],\n",
            "        [0.5100, 0.3500, 0.1400, 0.0200],\n",
            "        [0.5400, 0.3900, 0.1700, 0.0400],\n",
            "        [0.5000, 0.3300, 0.1400, 0.0200],\n",
            "        [0.5000, 0.2300, 0.3300, 0.1000],\n",
            "        [0.5700, 0.3800, 0.1700, 0.0300],\n",
            "        [0.6900, 0.3100, 0.5400, 0.2100],\n",
            "        [0.5100, 0.3800, 0.1600, 0.0200],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.6700, 0.3100, 0.4700, 0.1500],\n",
            "        [0.5500, 0.2400, 0.3700, 0.1000],\n",
            "        [0.5500, 0.2500, 0.4000, 0.1300],\n",
            "        [0.6900, 0.3200, 0.5700, 0.2300],\n",
            "        [0.7200, 0.3200, 0.6000, 0.1800],\n",
            "        [0.5200, 0.4100, 0.1500, 0.0100],\n",
            "        [0.6200, 0.2200, 0.4500, 0.1500],\n",
            "        [0.6800, 0.3000, 0.5500, 0.2100],\n",
            "        [0.5900, 0.3000, 0.5100, 0.1800],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.5500, 0.3500, 0.1300, 0.0200],\n",
            "        [0.6600, 0.2900, 0.4600, 0.1300],\n",
            "        [0.5900, 0.3200, 0.4800, 0.1800],\n",
            "        [0.6700, 0.3100, 0.4400, 0.1400],\n",
            "        [0.6500, 0.3000, 0.5500, 0.1800],\n",
            "        [0.6100, 0.2800, 0.4000, 0.1300],\n",
            "        [0.7000, 0.3200, 0.4700, 0.1400],\n",
            "        [0.6500, 0.3000, 0.5200, 0.2000],\n",
            "        [0.6100, 0.3000, 0.4600, 0.1400],\n",
            "        [0.5500, 0.4200, 0.1400, 0.0200]]) tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar os dados em treinamento e testes\n",
        "entrada_treinamento = entrada[0:120, :]\n",
        "saida_treinamento = saida[0:120]\n",
        "entrada_testes = entrada[120:150, :]\n",
        "saida_testes = saida[120:150]\n",
        "print(entrada_testes)\n",
        "print(saida_testes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZaa73CpqOLg",
        "outputId": "4bc9f73b-9bca-4d2c-bb08-1bdd7911f552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4600, 0.3100, 0.1500, 0.0200],\n",
            "        [0.5000, 0.3000, 0.1600, 0.0200],\n",
            "        [0.5100, 0.3500, 0.1400, 0.0200],\n",
            "        [0.5400, 0.3900, 0.1700, 0.0400],\n",
            "        [0.5000, 0.3300, 0.1400, 0.0200],\n",
            "        [0.5000, 0.2300, 0.3300, 0.1000],\n",
            "        [0.5700, 0.3800, 0.1700, 0.0300],\n",
            "        [0.6900, 0.3100, 0.5400, 0.2100],\n",
            "        [0.5100, 0.3800, 0.1600, 0.0200],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.6700, 0.3100, 0.4700, 0.1500],\n",
            "        [0.5500, 0.2400, 0.3700, 0.1000],\n",
            "        [0.5500, 0.2500, 0.4000, 0.1300],\n",
            "        [0.6900, 0.3200, 0.5700, 0.2300],\n",
            "        [0.7200, 0.3200, 0.6000, 0.1800],\n",
            "        [0.5200, 0.4100, 0.1500, 0.0100],\n",
            "        [0.6200, 0.2200, 0.4500, 0.1500],\n",
            "        [0.6800, 0.3000, 0.5500, 0.2100],\n",
            "        [0.5900, 0.3000, 0.5100, 0.1800],\n",
            "        [0.5800, 0.2700, 0.5100, 0.1900],\n",
            "        [0.5500, 0.3500, 0.1300, 0.0200],\n",
            "        [0.6600, 0.2900, 0.4600, 0.1300],\n",
            "        [0.5900, 0.3200, 0.4800, 0.1800],\n",
            "        [0.6700, 0.3100, 0.4400, 0.1400],\n",
            "        [0.6500, 0.3000, 0.5500, 0.1800],\n",
            "        [0.6100, 0.2800, 0.4000, 0.1300],\n",
            "        [0.7000, 0.3200, 0.4700, 0.1400],\n",
            "        [0.6500, 0.3000, 0.5200, 0.2000],\n",
            "        [0.6100, 0.3000, 0.4600, 0.1400],\n",
            "        [0.5500, 0.4200, 0.1400, 0.0200]])\n",
            "tensor([1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar o modelo para o treinamento\n",
        "print(entrada_treinamento.size())\n",
        "input_size = entrada_treinamento.size()[1]\n",
        "hidden_size = 10\n",
        "modelo = Net(input_size, hidden_size)\n",
        "print(modelo)\n",
        "\n",
        "# Configurações do modelo\n",
        "criterion = torch.nn.BCELoss() # Binary Cross Entropy\n",
        "criterion = torch.nn.MSELoss() # Mean Square Error\n",
        "optimizer = torch.optim.SGD(modelo.parameters(), lr = 0.9, momentum = 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4uV264Mqg1A",
        "outputId": "b1db9b5e-5c3c-4462-dcce-1d0a17a5c8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([120, 4])\n",
            "Net(\n",
            "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinar o modelo\n",
        "\n",
        "epochs = 10001 # Quantidade de épocas de treinamento\n",
        "\n",
        "errors = [] # Criando um array vazio para guardar os erros de cada epoca\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  # Forward pass\n",
        "  y_pred = modelo(entrada_treinamento)\n",
        "  #Compute Loss\n",
        "  loss = criterion(y_pred.squeeze(), saida_treinamento)\n",
        "  errors.append(loss.item())\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "  #Backward pass\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giFUNi_6qnFx",
        "outputId": "a9b31960-b71b-4b48-85ce-cc091fd2ddd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss: 0.29142168164253235\n",
            "Epoch 100: train loss: 0.011992462910711765\n",
            "Epoch 200: train loss: 0.0037464976776391268\n",
            "Epoch 300: train loss: 0.002128962893038988\n",
            "Epoch 400: train loss: 0.0014643599279224873\n",
            "Epoch 500: train loss: 0.0011062421835958958\n",
            "Epoch 600: train loss: 0.0008837238419800997\n",
            "Epoch 700: train loss: 0.0007326906197704375\n",
            "Epoch 800: train loss: 0.0006238037603907287\n",
            "Epoch 900: train loss: 0.0005417667562142015\n",
            "Epoch 1000: train loss: 0.00047786065260879695\n",
            "Epoch 1100: train loss: 0.00042675374425016344\n",
            "Epoch 1200: train loss: 0.00038500656955875456\n",
            "Epoch 1300: train loss: 0.00035030537401326\n",
            "Epoch 1400: train loss: 0.0003210313734598458\n",
            "Epoch 1500: train loss: 0.0002960250130854547\n",
            "Epoch 1600: train loss: 0.00027443014550954103\n",
            "Epoch 1700: train loss: 0.0002556061663199216\n",
            "Epoch 1800: train loss: 0.00023906076967250556\n",
            "Epoch 1900: train loss: 0.00022441231703851372\n",
            "Epoch 2000: train loss: 0.00021135798306204379\n",
            "Epoch 2100: train loss: 0.0001996555511141196\n",
            "Epoch 2200: train loss: 0.00018911020015366375\n",
            "Epoch 2300: train loss: 0.0001795614807633683\n",
            "Epoch 2400: train loss: 0.000170877217897214\n",
            "Epoch 2500: train loss: 0.0001629473699722439\n",
            "Epoch 2600: train loss: 0.00015567964874207973\n",
            "Epoch 2700: train loss: 0.0001489965507062152\n",
            "Epoch 2800: train loss: 0.00014283136988524348\n",
            "Epoch 2900: train loss: 0.0001371273392578587\n",
            "Epoch 3000: train loss: 0.0001318356953561306\n",
            "Epoch 3100: train loss: 0.0001269146305276081\n",
            "Epoch 3200: train loss: 0.00012232722656335682\n",
            "Epoch 3300: train loss: 0.00011804094538092613\n",
            "Epoch 3400: train loss: 0.00011402803647797555\n",
            "Epoch 3500: train loss: 0.00011026371066691354\n",
            "Epoch 3600: train loss: 0.00010672573262127116\n",
            "Epoch 3700: train loss: 0.00010339511936763301\n",
            "Epoch 3800: train loss: 0.00010025413212133572\n",
            "Epoch 3900: train loss: 9.728749137138948e-05\n",
            "Epoch 4000: train loss: 9.448150376556441e-05\n",
            "Epoch 4100: train loss: 9.182390931528062e-05\n",
            "Epoch 4200: train loss: 8.930323383538052e-05\n",
            "Epoch 4300: train loss: 8.690955291967839e-05\n",
            "Epoch 4400: train loss: 8.463397534796968e-05\n",
            "Epoch 4500: train loss: 8.246796642197296e-05\n",
            "Epoch 4600: train loss: 8.040384273044765e-05\n",
            "Epoch 4700: train loss: 7.843469938961789e-05\n",
            "Epoch 4800: train loss: 7.655435183551162e-05\n",
            "Epoch 4900: train loss: 7.475704478565603e-05\n",
            "Epoch 5000: train loss: 7.303734309971333e-05\n",
            "Epoch 5100: train loss: 7.139064837247133e-05\n",
            "Epoch 5200: train loss: 6.981244951020926e-05\n",
            "Epoch 5300: train loss: 6.829849735368043e-05\n",
            "Epoch 5400: train loss: 6.684524123556912e-05\n",
            "Epoch 5500: train loss: 6.544901407323778e-05\n",
            "Epoch 5600: train loss: 6.410682544810697e-05\n",
            "Epoch 5700: train loss: 6.281541573116556e-05\n",
            "Epoch 5800: train loss: 6.157206371426582e-05\n",
            "Epoch 5900: train loss: 6.037434650352225e-05\n",
            "Epoch 6000: train loss: 5.9219753893557936e-05\n",
            "Epoch 6100: train loss: 5.8106026699533686e-05\n",
            "Epoch 6200: train loss: 5.703104397980496e-05\n",
            "Epoch 6300: train loss: 5.5992866691667587e-05\n",
            "Epoch 6400: train loss: 5.498973041540012e-05\n",
            "Epoch 6500: train loss: 5.4020008974475786e-05\n",
            "Epoch 6600: train loss: 5.308191612130031e-05\n",
            "Epoch 6700: train loss: 5.217402576818131e-05\n",
            "Epoch 6800: train loss: 5.129509372636676e-05\n",
            "Epoch 6900: train loss: 5.044346835347824e-05\n",
            "Epoch 7000: train loss: 4.961821105098352e-05\n",
            "Epoch 7100: train loss: 4.881803033640608e-05\n",
            "Epoch 7200: train loss: 4.8041718400781974e-05\n",
            "Epoch 7300: train loss: 4.728834392153658e-05\n",
            "Epoch 7400: train loss: 4.6557124733226374e-05\n",
            "Epoch 7500: train loss: 4.584679118124768e-05\n",
            "Epoch 7600: train loss: 4.5156662963563576e-05\n",
            "Epoch 7700: train loss: 4.448587787919678e-05\n",
            "Epoch 7800: train loss: 4.383349005365744e-05\n",
            "Epoch 7900: train loss: 4.319904837757349e-05\n",
            "Epoch 8000: train loss: 4.258165790815838e-05\n",
            "Epoch 8100: train loss: 4.198070746497251e-05\n",
            "Epoch 8200: train loss: 4.139554584980942e-05\n",
            "Epoch 8300: train loss: 4.082561281393282e-05\n",
            "Epoch 8400: train loss: 4.027027898700908e-05\n",
            "Epoch 8500: train loss: 3.972909689764492e-05\n",
            "Epoch 8600: train loss: 3.9201262552523986e-05\n",
            "Epoch 8700: train loss: 3.868666317430325e-05\n",
            "Epoch 8800: train loss: 3.818461118498817e-05\n",
            "Epoch 8900: train loss: 3.7694841012125835e-05\n",
            "Epoch 9000: train loss: 3.7216697819530964e-05\n",
            "Epoch 9100: train loss: 3.6749883292941377e-05\n",
            "Epoch 9200: train loss: 3.6294044548412785e-05\n",
            "Epoch 9300: train loss: 3.5848661354975775e-05\n",
            "Epoch 9400: train loss: 3.541364276316017e-05\n",
            "Epoch 9500: train loss: 3.49884394381661e-05\n",
            "Epoch 9600: train loss: 3.457270940998569e-05\n",
            "Epoch 9700: train loss: 3.416615436435677e-05\n",
            "Epoch 9800: train loss: 3.3768661523936316e-05\n",
            "Epoch 9900: train loss: 3.337982343509793e-05\n",
            "Epoch 10000: train loss: 3.299922900623642e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teste = np.array([5.2,2.1,3.6,1.6])\n",
        "testetensor = torch.FloatTensor(teste)\n",
        "testetensor /= 10\n",
        "y_pred = modelo(testetensor)"
      ],
      "metadata": {
        "id": "XlgTFXvPlgeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rodar a rede com os dados de teste, os dados que a rede nunca viu\n",
        "#y_pred = modelo(entrada_testes)\n",
        "#print(y_pred) # valor previsto pela rede\n",
        "#print(saida_testes) # valor real"
      ],
      "metadata": {
        "id": "XNzsxwu1quIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def plotcharts(errors):\n",
        "    errors = np.array(errors)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    graf02 = plt.subplot(1, 2, 1) # nrows, ncols, index\n",
        "    graf02.set_title('Errors')\n",
        "    plt.plot(errors, '-')\n",
        "    plt.xlabel('Epochs')\n",
        "    graf03 = plt.subplot(1, 2, 2)\n",
        "    graf03.set_title('Tests')\n",
        "    a = plt.plot(saida_testes.numpy(), 'yo', label='Real')\n",
        "    plt.setp(a, markersize=10)\n",
        "    a = plt.plot(y_pred.detach().numpy(), 'b+', label='Predicted')\n",
        "    plt.setp(a, markersize=10)\n",
        "    plt.legend(loc=7)\n",
        "    plt.show()\n",
        "plotcharts(errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "2y00-_Zwqz-g",
        "outputId": "3a7fba44-ee10-42c0-d38f-d74037816586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}